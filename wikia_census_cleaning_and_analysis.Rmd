---
title: "Wikia census: cleaning and analysis"
author: "Abel Serrano Juste"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
bibliography: bibliography.bib
output:
  html_document:
    highlight: default
    theme: cosmo
    toc: yes
    toc_depth: 2
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
    toc: yes
lang: es
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r carga librerías, include=FALSE, echo=FALSE}
require("plyr") # para ordenar vectores
require("xtable") # para transfomar objetos R en tablas
```


# Introducción
En esta práctica, vamos a realizar un proyecto analítico de ciencia de datos sobre el ecosistema de las wikis.

Según la [definición de Wikipedia para wiki](https://es.wikipedia.org/wiki/Wiki):

> El término wiki (proviene del hawaiano wiki, «rápido») alude al nombre que recibe una comunidad virtual, cuyas páginas son editadas directamente desde el navegador, donde los mismos usuarios crean, modifican, corrigen o eliminan contenidos que, generalmente, comparten. 

Las wikis son un interesante objeto de estudio puesto que permiten investigar la colaboración masiva de usuarios online para crear un contenido común.

Utilizaremos los términos usuario y editor indistantemente, puesto que en el contexto de una wiki se pueden entender como sinónimos.

# Problema a resolver

Analizar la actividad y diversidad de las wikis alojadas en [el servicio Wikia](https://es.wikipedia.org/wiki/Fandom_(sitio_web)).

# Fuentes de datos

Para esta práctica, vamos a usar dos datasets: **Wikia census** y **Wikia page views**; ambos disponibles en [mi cuenta de kaggle](https://www.kaggle.com/abeserra/datasets).

1. [El censo de Wikia](https://www.kaggle.com/abeserra/wikia-census/#20181019-wikia_stats_users_birthdate.csv). Se trata de un dataset de un conjunto de 300k wikis que corresponde a todas las wikis alojadas en Wikia. Este dataset contiene datos descriptivos de cada wiki como: número de páginas, número de usuarios, número de ediciones, etc. Los métodos de extracción y la información proporcionada en este censo está explicada en el paper: [A Wikia census: motives, tools and insights](http://www.opensym.org/wp-content/uploads/2018/07/OpenSym2018_paper_27-1.pdf) [@jimenez2018opensym].

2. [Wikia page views](https://www.kaggle.com/abeserra/wikia-page-views). Se trata de una captura de datos, realizada mediante web scrapping de todas las wikis de Wikia, que contiene el número de visitas para cada una de las wikis de Wikia en las últimas cuatro semanas. Este dataset se obtuvo para la práctica anterior de esta misma asignatura y el código fuente para su obtención está en el este repositorio de Github: https://github.com/Akronix/scrap_wikia_page_views.


# Descripción del Dataset

Como hemos explicado previamente, vamos a usar dos datasets: el censo de wikia y los números de páginas visitadas.

## Wikia census dataset
El primer paso consistirá en cargar los datos:

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos el juego de datos
wikis<-read.csv("data/20181019-wikia_stats_users_birthdate.csv",header=T,sep=",")
```

A continuación, haremos una breve descripción de los datos, ya que nos interesa tener una idea general de los datos que disponemos. Para ello, primero calcularemos las dimensiones de nuestra base de datos y mostraremos una muestra de los datos para interpretar qué tipos de atributos tenemos.

```{r}
dim(wikis)
```

Disponemos de datos de 277795 wikis (filas) con 32 atributos sobre cada uno de ellos (columnas).

```{r}
head(wikis)
```

```{r}
str(wikis)
```

En base a la muestra, al conocimiento sobre el campo en el que estamos trabajando (wikis) y a la descripción proporcionada en el paper *"A Wikia census: motives, tools and insights"*, deducimos los siguientes atributos:

- url: url de la wiki
- creation_date: fecha de creación de la wiki en un timestamp
- domain: dominio web de la wiki
- founding_user_id: user id del fundador de la wiki
- headline: ??
- hub: Categoría de la wiki dentro de las definidas por Wikia.
- id: id de la wiki
- lang y language: idioma de la wiki
- name: Nombre propio de la wiki
- stats.activeUsers: número de usuarios activos en el último mes. Los usuarios activos son los usuarios que han hecho al menos una acción (una edición) en los últimos 30 días.
- stats.admins: número de usuarios administradores.
- stats.articles: número de artículos de la wiki.
- stats.discussions: ???
- stats.edits: número de ediciones en la wiki.
- stats.images: número de imágenes subidas.
- stats.pages: número de páginas de la wiki.
- stats.users: número de usuarios registrados en toda Wikia (potencialmente, cualquier podría user usuario de cada wiki porque los usuarios se registran a nivel de toda Wikia).
- stats.videos: número de videos subidos.
- title: título de la wiki
- topic: Temática de la wiki definida por el administrador de la wiki.
- wam_score: Puntuación que le da Wikia a las wikis: http://community.wikia.com/wiki/WAM/FAQ
- stats.nonarticles: número de páginas no artículos en la wiki.
- users_{1,5,10,20,50,100}: Número de usuarios con al menos {una, cinco, diez, veinte, cincuenta, cien} edición(es).
- bots: Número de usuarios de tipo bot (no humanos)
- birthdate: fecha de creación de la wiki en formato natural
- datetime.birthDate: fecha de creación de la wiki en formato datetime de Python

Para terminar con el estudio previo de los datos, pedimos a R que nos muestre un resumen de cómo están distribuidos los valores de los atributos:
```{r}
summary(wikis)
```

Los campos url y domain son identificadores de la wiki y deberían ser únicos. Aunque tenemos un repetido en el dominio cliff-side.wikia.com que trataremos más adelante.

Ahora podemos deducir que `headline` se refiere a una especie de subtítulo de la wiki. En cualquier caso, se trata de un campo de texto informativo para los usuarios de la wiki, pero que a nosotros no nos interesa.
El campo `stats.discussions` podría corresponderse con el número de páginas de discusión (o [talk pages](https://en.wikipedia.org/wiki/Help:Talk_pages)).

Tenemos muchísimas wikis sin videos y también muchas sin imágenes.

## Wikia page views dataset
Ahora procedemos a cargar los datos de visitas:
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos el juego de datos
wikis_pgv<-read.csv("data/20181113_wikia-page-views.csv", header=T, sep=",")
head(wikis_pgv)
```

Mostramos información descriptiva de estos datos:
```{r}
str(wikis_pgv)
summary(wikis_pgv)
```

Observamos que hay una wiki duplicada: `http://pl.6bp-6-batalion-pancerny.wikia.com/`.
También observamos que la mayoría de la wikis (más de la mitad) no han tenido ni una sola visita a sus páginas en las últimas cuatro semanas. Es decir, podríamos considerar que estas wikis están muertas, puesto que ni siquiera usuarios de internet externos a la comunidad las visitan.
En el otro extremo tenemos también otras wikis muy populares y así vemos como las medias que obtenemos de tanto páginas visitas como de visitas son mucho mayores que cero. Pero estas wikis que acumulan muchas visitas son escasas y no aparecen hasta más tarde del tercer cuartil de wikis.

Mostramos top 10 wikis con mayor número de visitas:
```{r}
head(wikis_pgv[with(wikis_pgv, order(desc(wikis_pgv$total_views))), ], n = 10)
```
Y top 10 wikis con mayor número de páginas visitadas:
```{r}
head(wikis_pgv[with(wikis_pgv, order(desc(wikis_pgv$visited_pages))), ], n = 10)
```


# Limpieza

Vamos a hacer limpieza de los datos que muestran valores raros o que no deberían estar:
Empezamos por tratar que el número de imágenes sea negativo:

Fijaremos a 0 cuando stats.images sea inferior a 0.
```{r}
wikis$stats.images[wikis$stats.images < 0] = 0
```

Fijaremos a 0 cuando stas.activeUsers sea inferior a 0. (Significa que no tenemos usuarios activos en esa wiki, pero no tiene sentido que tengamos valores menores que 0):
```{r}
wikis$stats.activeUsers[wikis$stats.activeUsers < 0] = 0
```


Eliminamos wikis con stats.users o stats.nonarticles o stats.pages menores que cero, puesto que más bien representan que la wiki no tiene datos válidos (una wiki normal al menos debe tener un usuario registrado o una página):
```{r}
invalid_wikis = wikis$stats.users < 0 | wikis$stats.pages < 0 | wikis$stats.nonarticles < 0
dim(wikis[invalid_wikis,]) # number of invalid wikis to delete
wikis = wikis[-invalid_wikis,]
dim(wikis)
```

Después, vemos qué pasa con los duplicados por dominio:
```{r}
wikis[duplicated(wikis$domain),]
#wikis[domain == "cliff-side.wikia.com", ] # Solo hay este dominio duplicado
# el dominio cliff-side.wikia.com está repetido. Eliminamos el último:
wikis = wikis[-duplicated(wikis$domain),]
dim(wikis)
```

Eliminamos también el duplicado que hemos visto para los datos de las visitas:
```{r}
wikis_pgv = wikis_pgv[!duplicated(wikis_pgv$url),]
summary(wikis_pgv)
```

# Transformación
En lugar de tener la fecha de creación del censo con formato fecha, que es un formato poco comparable y clasificable en intervalos, vamos a convertirlo a una nueva variable `age` que será la edad de la wiki en días:

```{r}
wikis$datetime.birthDate = as.POSIXct(wikis$datetime.birthDate)
#str(wikis$datetime.birthDate)
wikis$age = as.integer(Sys.time() - wikis$datetime.birthDate)
summary(wikis$age)
```

El valor máximo de 736808 corresponde al año 1 d.C., lo cual es imposible. Miramos cuántos valores de estos anómalos hay para age:
```{r}
wikis[wikis$age > 365 * 22 , c('birthDate', 'datetime.birthDate', 'age')] # wikis con más de 22 años
```
Observamos que hay un error en la wiki con url: http://jrmime.wikia.com. La fecha en `birthDate` es incorrecta (año 1), mientras que la fecha de creación en realidad es 2013-03-11.

Lo arreglamos:
```{r}
aux = wikis[wikis$age > 365 * 22 ,]
aux$age = as.integer(Sys.time() - as.POSIXct(aux$creation_date))
wikis[wikis$age > 365 * 22 ,] = aux
# Comprobamos de nuevo si hay algún otro caso raro:
wikis[wikis$age > 365 * 22 ,] # wikis con más de 22 años
summary(wikis$age)
```


# Integración
Ahora vamos a unir los dos datasets de los cuales disponemos: Wikia Census y Wikia page views. Es lo podemos hacer añadiendo los datos de visitas al dataframe `wikis` que ya tenemos. Para ello debemos juntar ambos dataframes usando como columna identificadora común la columna `url`.

```{r}
wikis_all = merge(wikis, wikis_pgv, by="url")
dim(wikis_all)
summary(wikis_all)
```

# Transformación

Definimos una wiki como inactiva cuando el número de usuarios activos en el último mes (atributo `stats.activeUsers`) es igual a cero. Esto nos resultará útil para el posterior paso de análisis.

Para ello, creamos una nueva columna llamada `active` que será TRUE si stats.activeUsers > 0 o FALSE en caso contrario.

```{r}
wikis_all$active = wikis_all$stats.activeUsers > 0
str(wikis_all$active)
```

Similarmente, creamos una columna `visited` que será TRUE si ha habido alguna visita en el último mes en la wiki, o FALSE en caso contrario.

```{r}
wikis_all$visited = wikis_all$total_views > 0
str(wikis_all$visited)
```

# Análisis
Un análisis que me resulta interesante es ver si hay correlación entre ciertas variables y la actividad/inactividad de las wikis.

Primero, vamos a ver si hay correlacción lineal entre el número de usuarios activos y el número de visitas en el último mes:
```{r}
regmodel <- lm( stats.activeUsers ~ total_views + visited_pages, data = wikis_all)
summary(regmodel)
```

Los resultados muestran que ambas variables: total_views y visited_pages son variables explicativas para determinar el número de usuarios activos.

Y viceversa: ¿El número de visitas determina el número de usuarios activos?
```{r}
regmodel <- lm( total_views ~ stats.activeUsers, data = wikis_all)
summary(regmodel)
```

Efectivamente, la variable usuarios activos también determina si la wiki recibe visitas o no, por lo que podemos pensar que la wiki esté activa porque hay editores colaborando es equivalente a que tenga interés / utilidad para el resto del mundo.

A continuación, vamos a ver qué factores determinan que una wiki esté activa/inactiva. Usamos un modelo de regresión logística y seleccionamos un subconjunto de variables que nos resulten relevantes para este análisis:

```{r}
RELEVANT_ATTRS = c("hub", "language", "stats.articles", "stats.admins", "stats.edits", "stats.pages", "wam_score", "stats.pages", "users_1", "users_5", "users_10", "users_20", "users_50", "users_100")
formula <- as.formula(paste("active ~ ", paste(RELEVANT_ATTRS, collapse = "+") ))
regmodel.1 <- glm( formula = formula, family = binomial(link = 'logit'), data = wikis_all)
summary(regmodel.1)
```
Los atributos que influyen a la hora de determinar que la wiki esté activa o no son (sin ordenar por importancia): wam_score, hubOther, stats.edits, hubTV, hubGames, hubComics, hubLifestyle, hubMovies, stats.articles, stats.admins, stats.pages, users_1, users_5, users_100, users_50.

hubMusic y users_20 influyen muy poco y ya están bien representados con las anteriores variables, así que consideraremos que no son relevantes para nuestro modelo.

Ordenamos los atributos por importancia:

```{r}
idx <- order(coef(summary(regmodel.1))[,4])  # sort out the p-values
out <- coef(summary(regmodel.1))[idx,]       # reorder coef, SE, etc. by increasing p
xtable(out)
```

Repetimos el análisis pero ahora usando como indicador de actividad que la wiki haya tenido alguna visita en el último mes:

```{r}
formula <- as.formula(paste("visited ~ ", paste(RELEVANT_ATTRS, collapse = "+") ))
regmodel.2 <- glm( formula = formula, family = binomial(link = 'logit'), data = wikis_all)
summary(regmodel.2)
```


# Visualización
Mostramos cómo son las correlacciones que hemos realizado en el paso anterior:
```{r}
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(regmodel.1, las = 1)      # Residuals, Fitted, ...
par(opar)
```


```{r}
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(regmodel.2, las = 1)      # Residuals, Fitted, ...
par(opar)
```

<!-- # Conclusiones -->

# Resultados
Finalmente, vamos a producir un fichero con los datos ya limpiados, transformados e integrados: 
```{r}
write.csv(wikis_all, "output_data/20181019-wikia_census_and_page_views-clean.csv")
```




# Referencias